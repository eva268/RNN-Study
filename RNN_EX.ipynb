{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 필요한 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 종류 확인\n",
    "def build_vocab_from_df(df: pd.DataFrame):\n",
    "    char_vocab = set()\n",
    "    for line in df[\"msg\"]:\n",
    "        for c in line:\n",
    "            char_vocab.add(c)\n",
    "    return char_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (3618, 2)\n",
      "val: (775, 2)\n",
      "test: (776, 2)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "val = pd.read_csv(\"data/val.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "print(\"train:\",train.shape)\n",
    "print(\"val:\",val.shape)\n",
    "print(\"test:\",test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문자 종류 갯수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "[' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '~', '£', '…']\n",
      "88\n",
      "[' ', '!', '\"', '#', '$', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '£', '…']\n",
      "88\n",
      "[' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '£', '…']\n",
      "set()\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "train_vocab = build_vocab_from_df(train)\n",
    "val_vocab = build_vocab_from_df(val)\n",
    "test_vocab = build_vocab_from_df(test)\n",
    "print(len(train_vocab))\n",
    "print(sorted(train_vocab))\n",
    "print(len(val_vocab))\n",
    "print(sorted(val_vocab))\n",
    "print(len(test_vocab))\n",
    "print(sorted(test_vocab))\n",
    "\n",
    "print(set(test_vocab) - set(train_vocab))\n",
    "print(set(val_vocab) - set(train_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vocab 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'pad', 1: 'unk', 2: 'P', 3: '!', 4: \"'\", 5: 'L', 6: '\"', 7: 'z', 8: 'Z', 9: '.', 10: 'r', 11: '[', 12: 'K', 13: '1', 14: 'w', 15: '…', 16: '&', 17: 'p', 18: 'V', 19: '\\\\', 20: 'J', 21: '@', 22: '#', 23: 'O', 24: 'S', 25: '?', 26: 'M', 27: '$', 28: 'F', 29: 'b', 30: 'T', 31: 'W', 32: '7', 33: ']', 34: 'G', 35: '=', 36: '5', 37: 'N', 38: 'o', 39: 'B', 40: '%', 41: 'x', 42: '(', 43: 'q', 44: 'D', 45: 'I', 46: 'i', 47: 'C', 48: ',', 49: 'U', 50: ' ', 51: 'h', 52: 'g', 53: 'A', 54: 'c', 55: '9', 56: '~', 57: ':', 58: ')', 59: '*', 60: 'l', 61: '-', 62: 'H', 63: 'y', 64: '|', 65: 'd', 66: 's', 67: '+', 68: 'X', 69: '0', 70: '6', 71: '8', 72: 'v', 73: 'Y', 74: '<', 75: 'k', 76: 'u', 77: '3', 78: 'a', 79: 'f', 80: 'R', 81: 'Q', 82: '4', 83: '_', 84: '/', 85: 'm', 86: 'e', 87: 'n', 88: 'j', 89: '>', 90: '£', 91: '^', 92: 'E', 93: '2', 94: ';', 95: 't'}\n",
      "vocab size: 96\n"
     ]
    }
   ],
   "source": [
    "char2idx = dict()\n",
    "char2idx[\"pad\"] = 0\n",
    "char2idx[\"unk\"] = 1\n",
    "for index, char in enumerate(train_vocab):\n",
    "    char2idx[char] = index + 2\n",
    "\n",
    "idx2char = {value: key for key, value in char2idx.items()}\n",
    "print(idx2char)\n",
    "print(\"vocab size:\", len(char2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n"
     ]
    }
   ],
   "source": [
    "seq_len = train[\"msg\"].str.len().max()\n",
    "input_length = len(char2idx)\n",
    "print(seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 커스텀 데이터셋 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data=train):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[\"msg\"][idx]\n",
    "        y = self.data[\"label\"][idx]\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토크나이저 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string: str):\n",
    "    x_data = [char2idx[c] for c in string]\n",
    "    return torch.tensor(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collate function 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(samples):\n",
    "    x = [i[0] for i in samples]\n",
    "    x = [tokenize(i) for i in x]\n",
    "    x = pad_sequence(x, batch_first=True)\n",
    "    \n",
    "    y = [i[1] for i in samples]\n",
    "    y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Cell과 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNCell_Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size):\n",
    "        super(RNNCell_Encoder, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.rnn = nn.RNNCell(input_dim, hidden_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        ht = torch.zeros(self.hidden_size)\n",
    "\n",
    "        for character in input:\n",
    "            ht = self.rnn(character, ht)\n",
    "        return ht\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_size):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.em = nn.Embedding(len(char2idx), embedding_dim)  # 임베딩\n",
    "        self.rnn = RNNCell_Encoder(embedding_dim, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, 1)\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.em(x)\n",
    "        x = x.squeeze(0)\n",
    "        x = self.rnn(x)\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        # x = F.relu(self.fc1(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 하이퍼파라미터 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(2, 50)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epoch, model, trainloader, valloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for x, y in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred = torch.argmax(y_pred, dim=0)\n",
    "            correct += (y_pred == y).sum().item()\n",
    "            total += y.size(0)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_running_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in valloader:\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            y_pred = torch.argmax(y_pred, dim=0)\n",
    "            val_correct += (y_pred == y).sum().item()\n",
    "            val_total += y.size(0)\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "    epoch_val_loss = val_running_loss / len(valloader)\n",
    "    epoch_val_acc = val_correct / val_total\n",
    "\n",
    "    print(f\"train_loss :{epoch_loss:.6f} train_acc :{epoch_acc:.6f} val_loss :{epoch_val_loss:.6f} val_acc :{epoch_val_acc:.6f}\")\n",
    "\n",
    "    return epoch_loss, epoch_acc, epoch_val_loss, epoch_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss :0.433509 train_acc :0.873687 val_loss :0.374697 val_acc :0.873548\n",
      "epoch 0 finished in 21.77 seconds\n",
      "train_loss :0.375332 train_acc :0.873687 val_loss :0.373275 val_acc :0.873548\n",
      "epoch 1 finished in 21.59 seconds\n",
      "train_loss :0.374057 train_acc :0.873687 val_loss :0.371781 val_acc :0.873548\n",
      "epoch 2 finished in 21.57 seconds\n",
      "train_loss :0.372934 train_acc :0.873687 val_loss :0.370693 val_acc :0.873548\n",
      "epoch 3 finished in 21.56 seconds\n",
      "train_loss :0.371854 train_acc :0.873687 val_loss :0.369035 val_acc :0.873548\n",
      "epoch 4 finished in 21.60 seconds\n",
      "train_loss :0.385537      train_acc :0.873687        val_loss :0.371896        val_acc :0.873548\n",
      "training finished in 108.09 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "train_ds = MyDataset(train)\n",
    "val_ds = MyDataset(val)\n",
    "\n",
    "epochs = 5\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    train_iterator = iter(\n",
    "        DataLoader(train_ds, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "    )\n",
    "    val_iterator = iter(\n",
    "        DataLoader(val_ds, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "    )\n",
    "\n",
    "    epoch_loss, epoch_acc, epoch_val_loss, epoch_val_acc = training(\n",
    "        epoch, model, train_iterator, val_iterator\n",
    "    )\n",
    "\n",
    "    train_loss.append(epoch_loss)\n",
    "    train_acc.append(epoch_acc)\n",
    "    val_loss.append(epoch_val_loss)\n",
    "    val_acc.append(epoch_val_acc)\n",
    "\n",
    "    epoch_end = time.time()\n",
    "    print(f\"epoch {epoch} finished in {epoch_end-epoch_start:.2f} seconds\")\n",
    "\n",
    "end = time.time()\n",
    "print(f\"train_loss :{sum(train_loss)/epochs:.6f}\\\n",
    "      train_acc :{sum(train_acc)/epochs:.6f}\\\n",
    "        val_loss :{sum(val_loss)/epochs:.6f}\\\n",
    "        val_acc :{sum(val_acc)/epochs:.6f}\")\n",
    "\n",
    "print(f\"training finished in {end-start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.3739169838924691      test accuracy: 0.8737113402061856\n"
     ]
    }
   ],
   "source": [
    "test_ds = MyDataset(test)\n",
    "test_iterator = iter(DataLoader(test_ds, batch_size=1, collate_fn=collate_fn))\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "test_loss = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y in test_iterator:\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        y_pred = torch.argmax(y_pred, dim=0)\n",
    "        if y_pred == 1:\n",
    "            print('y is spam')\n",
    "        test_correct += (y_pred == y).sum().item()\n",
    "        test_total += y.size(0)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "test_loss = test_loss / len(test_iterator)\n",
    "test_acc = test_correct / test_total\n",
    "print(f'test loss: {test_loss}\\\n",
    "      test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8737113402061856"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test[\"label\"] == 0).sum() / test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 데이터포인트에 대해 ham을 예측했다.  \n",
    "낮은 성능의 이유로는 문자 레벨 토큰화로 인해 너무 길어진 시퀀스 길이, RNNCell 사용으로 배치 훈련 불가 등으로 추측된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
